[{"id":"da5f341d43fdcb48713cf189ba6fcbc4","title":"CodeQL相关论文整理(持续更新中)","content":"前排提醒，本文中与codeql相关的论文为按照时间线整理，数据来源为：https://codeql.github.com/publications/ 论文整理仅为自用，可能有错漏之处，还望批评指正。\n2007年：QL for Source Code Analysis.（https://codeql.github.com/publications/ql-for-source-code-analysis.pdf）\n这篇论文是官网中所提到的第一篇论文，这里论文作者在一开头就解释了：\n（1）CodeQL（当时还叫.QL）为什么要采用类SQL语句？即单纯为了降低开发者的学习障碍，且这种相似性只在语法上。\n（2）CodeQL会被编译成什么语句? CodeQL最后会被编译为DataLog语句，因为datalog的查询是可以递归的，这对查询继承层次结构或调用图非常重要。\n（3）CodeQL使用的是Eindhoven Quantifier Notation（埃因霍温量化符表示法），一种优雅的计算某些度量属性的方法\n（4）面向对象查询语言的语义模型应该怎么设计：.QL 采取了一个看似简单且一致的观点，即类是谓词，继承是蕴含。这种观点具有简单性的优势，但严格应用这些原则会导致一些初看起来令人惊讶的后果，特别是需要非确定性表达式。然而，允许非确定性表达式确实使得查询更加简洁，从而改善了语言设计。\n除此之外，论文还介绍了CodeQL的早期版本：SemmleCode。其为使用abc代码库为例子进行测试\n整个查询过程可以分为以下几个阶段：\n（1）选择语句\n这里要注意几点。第一，这个阶段本身可以进行路径无关的Bug的检测。第二，与sql的select语句相比，变量在使用前被声明。\n（2）谓词（Predicates）\n可以类比为codeql对对象操作的”函数“。例如（以下论文原文）：\n查询可以通过将它们包装在谓词中来命名和参数化。为了说明这一点，我们在 abcplus 中构建两个给定类型之间的所有类型的图。首先，我们定义一个测试，以检查从下层（down）通过中间（between）到上层（up）的路径在层次结构中是否存在：\n1234predicate between(RefType down, RefType between, RefType up) &#123;    down.hasSupertype∗(between) and    between.hasSupertype∗(up)&#125;\n\n在这里，t1.hasSupertype(t2) 是一个测试，用于确定 t1 是否有直接的超级类型 t2。表达式 t1.hasSupertype∗(t2) 中的星号表示间接性：存在从 t1 到 t2 的超级类型链（可能长度为零）。\n现在，这个谓词可以用于另一个查询，以找到从 abc.aspectj.ast.AdviceDecl c（表示建议声明）到 polyglot.ast.Node 的所有路径：\n123456789from RefType c1, RefType c2,RefType node, RefType adviceDeclwhere    node.hasQualifiedName(&quot; polyglot . ast&quot;,&quot;Node&quot;) and    adviceDecl.hasQualifiedName(&quot;abc. aspectj . ast&quot;,&quot;AdviceDecl c&quot;) and    between(adviceDecl,c1,node) and    between(adviceDecl,c2,node) and    c2.hasSupertype(c1)select c1,c2\n\n（3）聚合\n这里直接使用了埃因霍温量化符表示法，让推理证明变得简洁。例如：\n12345from Package pwhere p.hasName(&quot;abc.aspectj. ast&quot;)select sum(CompilationUnit cu |    cu.getPackage()=p |    cu.getNumberOfLines())\n\n\n\n（4）类\nCodeql支持面向对象，可以编写自己的类\n在这篇文章的版本中 codeql还没有引入控制流\n2007 QL: Object-Oriented Queries Made Easy.（要钱，没看）(https://pdfs.semanticscholar.org/d7be/52e78154dbde1884f07510fa7901b9915277.pdf)\n\n\n2008 Adding Magic to an Optimising Datalog Compiler(https://dl.acm.org/authorize?N84086)\n这里，codeql团队尝试了使用.ql转化为datalog，之后再转化为sql.\n2008 Type Inference for Datalog and Its Application to Query Optimisation.(https://dl.acm.org/authorize?N84087)\n这篇文章中，codeql团队尝试了使用非笛卡尔类型的方法进行类型推断的效果。\n2010 Type Inference for Datalog with Complex Type Hierarchies.(https://dl.acm.org/doi/10.1145/1707801.1706317?cid=81381592942)\nDatalog 的类型推断可以理解为将程序映射到一个子语言的问题，该子语言的包含性是可判定的。具体来说，给定一个 Datalog 程序、一个描述扩展关系类型的模式，以及用户提供的关于基本类型的基本事实（声明条件，如不相交性、蕴含或等价性），我们旨在推断出程序语义的近似表示，这个表示应该可以用 Datalog 的一个合适的子语言来表达。\n我们主张 Datalog 加上单值扩展（monadic extensionals）是那个类型子语言的一个合适选择，并介绍了推断算法。该推断算法被证明是可靠的，我们还展示它为一大类 Datalog 程序推断出了尽可能紧致的近似表示。此外，我们为我们的类型语言的大一部分呈现了一个实用的包含性检查。这个包含性检查的核心是一种对 Quine 计算原蕴涵（prime implicants）程序的新颖推广。该类型系统已在最先进的工业数据库系统中实现，我们报告了关于这一实现的实验。\n2015 Tracking Static Analysis Violations Over Time to Capture Developer Characteristics.(https://codeql.github.com/publications/tracking-analysis-violations.pdf)\n我们提出了一种方法，用于跟踪程序的修订历史中静态分析违规（通常表明缺陷）的情况，并精确地将这些违规的引入和消除归因于个别开发者。\n2016 QL: Object-oriented Queries on Relational Dat（CodeQL中最具代表性的部分）(http://drops.dagstuhl.de/opus/volltexte/2016/6096/pdf/LIPIcs-ECOOP-2016-2.pdf)\n开始出现CFG\n本文描述了QL，一种查询复杂、可能递归的数据结构的查询语言。QL编译成Datalog并在标准关系数据库上运行，但它提供了类似的对象导向特性，如类和方法，这些特性在逻辑术语中被重新解释：类是描述值集的逻辑属性，子类化是蕴含，虚拟调用是通过考虑接收者包含的最具体类来动态分派。此外，QL中的类型是规定性的，并积极影响程序评估，而不仅仅是描述它。结合这些特性，可以基于可重用库开发简洁的查询，这些库以纯声明式风格编写，但即使在大数据集上也能高效执行。特别是，我们使用QL为各种编程语言实现了静态分析，这些分析可以扩展到数百万行代码。\n2017 Algebraic Data Types for Object-oriented Datalog.(https://codeql.github.com/publications/algebraic-data-types.pdf)\nDatalog 是一种流行的语言，用于实现程序分析：不仅因为它是一种优雅的正式化，可以简洁地指定最小固定点算法，这是程序分析的基本组成部分，而且这些声明式规范也可以高效执行。然而，纯 Datalog 只能处理原子值，并没有为任何类型的结构化数据提供第一类支持。这使得需要甚至非常简单的数据结构（如对偶）的算法表达起来非常繁琐，而对于需要树或列表的算法来说则完全不可能。因此，非平凡分析往往依赖于允许在空中创建新值来表示复合数据的额外逻辑特性。我们提出了一种更高级的解决方案：我们扩展了 QL，一种面向对象的 Datalog 方言，并引入了一种代数数据类型的概念，它提供了通常的组合、不相交并集和递归。此外，代数数据类型的分支可以是完整的 QL 谓词，它们不仅可以与其他数据类型递归，还可以与任意的其他谓词递归，从而能够对数据类型的结构进行非常细粒度的控制。新的类型与 QL 现有的类和虚拟分派概念无缝集成，后者扮演着模式匹配构建的角色。我们通过扩展 QL 评估器，添加了一个在运行时创建新值的低级操作符，并将代数数据类型翻译为应用此操作符的方法。为了展示我们方法的实际有用性，我们讨论了三个案例研究，这些案例研究涉及 QL 之前难以或无法解决的一般程序分析问题。\n2018 Measuring software development productivity: a machine learning approach.(https://codeql.github.com/publications/measuring-software-development.pdf)\n开始尝试机器学习的在静态分析的应用\n2019 The standard coder: a machine learning approach to measuring the effort required to produce source code change.(https://arxiv.org/abs/1903.02436)\n进一步尝试机器学习的在静态分析的应用\n2019 Unsupervised Recalibration.(https://arxiv.org/pdf/1908.09157.pdf)\n无监督重新校准（URC）是一种通用的方法，可以在现场部署的已经训练好的概率分类或回归模型遇到新数据时提高其准确性。URC 无需与新现场数据相关联的任何真实值。URC 仅观察模型的预测，并注意到预测的预期分布与实际分布之间的差异，从而识别出训练数据集可能不具代表性。它通过反向工作来确定偏差的大小，然后将其消除。当应用于现场观察到的不同子群体时，URC 特别有用，这些子群体在训练机器学习模型时并未被视为特征。这使得可以利用子群体的信息，而无需重新训练模型，甚至对于某些或所有子群体无需真实值。此外，如果这些子群体是研究对象，URC 有助于确定它们的正确真实值分布，其中简单的聚合方法，如平均模型的预测，系统性地低估了它们之间的差异。\n","slug":"CodeQL相关论文整理","date":"2024-04-24T13:25:17.132Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"33d3a0a2cbefdcf717ea2b53096214d6","title":"LLM_LangChain八股文（整理自用_不保证正确）（持续更新）","content":"LangChain本身什么是 LangChain?LangChain 是一个开发由语言模型驱动的应用程序的框架。我们相信最强大和不同的应用程序不仅会通过 API 调用语言模型， 还会：\n数据感知 : 将语言模型连接到其他数据源具有代理性质 : 允许语言模型与其环境交互\nLangChain 中 Components and Chains 是什么？Components（组件）\n“Components”在LangChain中指的是单个的、可重用的功能模块，每个组件都是独立的，并执行特定的任务。这些组件可以是预处理文本的工具、执行特定查询的功能、与外部API交互的接口，或者是直接与语言模型交互的部分。组件是高度模块化的，可以在不同的应用中以不同的方式组合使用。\n组件的主要类别包括：\nLanguage Model Components: 与特定的语言模型交互，如生成文本、回答问题等。Preprocessing Components: 对输入数据进行预处理，如文本清洗、格式化等。Postprocessing Components: 对模型的输出进行后处理，如修正语法错误、过滤不当内容等。Integration Components: 与外部服务或API集成，比如获取外部数据、调用其他AI服务等。\nChains（链）\n“Chains”是多个组件按照特定的顺序连接起来形成的处理流水线。每个链定义了数据如何从一个组件传递到另一个组件，以及如何在整个应用中流动。链允许开发者将简单的组件组合成复杂的工作流程，无需关心各个组件之间的具体交互细节。\n链的设计使得开发者可以灵活地定义组件之间的数据流向和处理逻辑，实现从数据输入到最终输出的全过程控制。这种方式不仅提高了开发效率，还增强了应用的可维护性和扩展性。\n2.2 LangChain 中 Prompt Templates and Values 是什么？在 LangChain 中，Prompt Templates 和 Values 是构建有效的语言模型查询（prompting）过程中的两个关键组件。它们协助在不同的上下文和需求下构造和定制化 prompt 的内容，以便从语言模型中获取最准确和相关的回答或输出。\nPrompt Templates（提示模板）\nPrompt Templates 是预定义的文本模板，用于生成向语言模型提交的完整提示（prompts）。这些模板通常包含占位符（placeholders），它们在实际使用时会被具体的值替换。使用模板的目的是为了确保提示的一致性、可控性和可复用性，同时也使得提示的创建过程更加自动化和规范化。\n模板的设计允许开发者预先定义和优化与语言模型交互的方式，通过精心设计的语言和结构来引导模型生成特定类型的回答或内容。这种方法在需要对模型输出进行精细控制时尤其有用，例如在需要遵循特定格式或风格的应用场景中。\nValues（值）\nValues 是在实际生成 prompt 时填充到 Prompt Templates 中的具体数据或信息。这些值可能来自用户输入、数据库查询结果或任何其他数据源。值的选择和处理是动态进行的，取决于具体的应用场景和需求。\n使用值的目的是使得生成的 prompt 能够针对具体的情况或问题进行定制，提高模型回答的相关性和准确性。例如，在一个为用户提供个性化建议的应用中，每个用户的输入和背景信息都会作为值被用来生成定制的 prompt。\n在 LangChain 中，Example Selectors 是指用于从一个更大的数据集中选择特定实例（例子）以用于构建 prompt 的组件。这一功能在那些需要动态地根据上下文或用户输入来生成具体且相关的提示文本时尤为重要。Example Selectors 通常用于增强提示的相关性和效果，尤其是在那些需要模型考虑多个数据点以做出最佳回答的场景中。\n功能和用途Example Selectors 的主要功能是筛选和选择最适合当前查询需求的数据实例。在处理大量数据或信息时，不可能也不必要将所有数据一次性提供给模型。相反，选择最相关的信息以构建有效的 prompt 是提高模型性能的关键。\n这些选择器可以基于多种标准进行工作，例如：\n相关性：选择与当前查询最相关的实例。最新性：优先选择最新的数据实例。多样性：确保选出的实例在某些特征上具有代表性或多样性。特定规则：基于预设的业务规则或逻辑来选择实例。\nLangChain 中 Example Selectors 是什么？LangChain 是一个开源的库，用于构建和部署基于语言模型的应用。它提供了一种结构化的方式来整合不同的组件，以创建复杂的语言理解和生成任务。在 LangChain 中，Example Selectors 是一个特定的组件，用于从一组给定的示例中选择最合适的示例来回答用户的问题或完成特定的任务。\n功能和目的Example Selectors 的主要目的是改善语言模型在特定任务上的表现，通过精选最相关的信息或示例来引导模型的输出。这种方法特别有用在处理那些需要基于既有知识或数据进行回答的场景，例如，法律咨询、技术支持或任何专业领域的咨询。\nLangChain 中 Output Parsers 是什么？在 LangChain 中，Output Parsers 是一个重要的组件，用于处理和解释语言模型生成的输出。这个组件的主要功能是将模型的原始输出转换成结构化的数据或更加具体、易于理解的格式。Output Parsers 在 LangChain 应用中起到了桥接模型输出与最终用户需求的关键作用。\n功能和目的Output Parsers 的核心目的是从模型生成的通常较为自由形式的文本中提取有用信息，并将其转化为更加有序或定制化的格式。这对于实现自动化流程、数据分析、用户交互等多种应用场景非常关键。\nLangChain 中 Indexes and Retrievers 是什么？在 LangChain 中，Indexes and Retrievers 是两个关键组件，它们共同作用于信息检索的环节，帮助从大量的数据中快速准确地找到与用户查询相关的信息。这些组件对于构建基于知识的应用、问答系统等非常重要，因为它们提供了从大数据集中提取信息的能力。\nIndexes（索引）Indexes 是指将数据组织、存储和索引的方式，使得可以高效地检索信息。索引通常涉及数据的预处理，如文本的分词、标准化、向量化等，以便于快速查找。在 LangChain 中，索引可以是简单的关键词索引，也可以是更复杂的向量空间模型，如使用BERT或其他语言模型生成的嵌入向量。\nLangChain 中 Chat Message History 是什么？在 LangChain 中，Chat Message History 是指在创建和管理对话应用时使用的一种机制，用于记录和跟踪与用户之间交互的历史信息。这个组件的功能是保存以前的交流记录，使得在处理当前和未来的用户输入时，模型可以利用这些历史信息来生成更加准确和连贯的回答。\n使用Langchain链式调用\nLLMChain是单个调用 SimpleSequentialChain是多个简单顺序调用 也可以创建自定义链 \n路由链RouterChain\n路由链（RouterChain）用于创建可以动态选择下一条链的链。\nRouterChain 由两个部分组成:\n路由链本身(负责选择下一条链)destination_chains：RouterChain 可以路由到的链RouterChain 的几种类型包括:\nLLMRouterChain：使用一个语言模型来决定如何路由EmbeddingRouterChain：使用嵌入和相似性来路由到不同的链RouterChain 通常与其他链组合使用，比如 MultiPromptChain，可以根据问题动态选择不同的 prompt 来回答问题。\n总之，RouterChain 允许链动态地选择下一步的操作，实现更加智能和灵活的链。它是 Langchain 中实现链组合的重要组件之一。\n转换链（TransformChain）允许在链之间添加自定义的转换函数。\nTransformChain 的主要组成部分是:\ninput_variables：输入变量名列表output_variables：输出变量名列表transform：自定义的转换函数\n转换函数接受一个字典作为输入，字典中的键是 input_variables 中定义的变量名。\n转换函数需要返回一个字典，字典中的键是 output_variables 中定义的变量名。\n这样 TransformChain 就可以在链之间添加任意的转换逻辑，如清理、过滤、格式化数据等。\n总之，TransformChain 为在链之间添加自定义转换提供了一种简单的方法，使链之间的数据流更加灵活。\n文档链 DocumentsChain文档链（DocumentsChain）用于将多个文档作为输入传递给下游的链。它可以用来从多个文档中抽取信息、进行问答、总结等任务。\nDocumentsChain 的一些关键特点:\n可以将多个文档合并成一个虚拟的大文档，传递给下游链。支持从文档中抽取关键词，命名实体等信息。可以针对每个文档单独处理，然后合并结果。支持根据文档内容进行路由，选择不同的下游链。可以跟踪每个结果来自哪个文档。支持各种文档格式，如文本、PDF、HTML等。DocumentsChain 通常与问答链、总结链等结合使用,来利用多个文档的信息。它简化了处理多个输入文档的流程。\n总之，DocumentsChain 是 LangChain 中处理多文档输入的重要组件，允许构建更加智能的链式模型。\n什么是 LangChain Agent?代理 Agents\n代理（Agents）的核心思想是使用 LLM 作为大脑自动思考，自动决策选择执行不同的动作，最终完成我们的目标任务。\nAgents 模块有几个关键组件：\n代理 Agent\n代理（agent）底层的实现方式是通过 LLM 来决定下一步执行什么动作，从而扮演着我们的代理角色。它可以协助我们做出决策，调用相应的 API，为我们提供高效便捷的服务。\n工具 Tools\n工具（Tools）是代理调用的函数或 API，需要被正确调用并以最有帮助的方式描述，以便代理能够顺利运行。LangChain 提供了广泛的入门工具，但也支持定义自己的工具，包括自定义描述。\n工具集 Toolkits\n工具集（Toolkits）通常提供给LLM的工具不仅仅只有一两个，而是一组可供选择的工具集，这样可以让 LLM 在完成任务时有更多的能力和选择。此外，这些工具集还可能包含完成特定目标所需的工具组合。\n代理执行器 AgentExecutor\n代理执行器（AgentExecutor）是在代理运行时执行的，它的作用是为您处理代理在选择工具时可能遇到的一些问题，比如代理选择不存在的工具或者工具出现错误等情况。此外，代理执行器还可以处理代理生成的输出无法解析为工具调用的情况，并在所有级别（包括代理决策和工具调用）上进行日志记录和可观察性输出。这样可以更好地保证系统的稳定性和可靠性。\n回调 Callbacks\nLangChain 提供了一个回调系统，允许您连接到 LLM 申请的各个阶段。这对于日志记录、监控、流传输和其他任务（添加标签、计算 Token 等）非常有用\n","slug":"LLM_langchain八股文","date":"2024-04-14T14:21:58.196Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"542f169762b7aca64f020ec0392added","title":"LLM八股文（整理自用_不保证正确）（持续更新）","content":"Attention改进方面Flash_Attention（这一部分主要参考链接：https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh 感谢杨同学）\n在传统的Attention中，softmax(QK^T^&#x2F;dmk^1&#x2F;2^)中的矩阵运算很大，需要在HBM中实例化存储，因此会带来很多的对显存的访存操作，导致耗时较长。\nFlashAttention则是将对QK的softmax计算进行了分片，每次进行部分计算时。计算都能在SRAM中进行交互，从而减少对显存的依赖。\n让我们将softmax(QK^T^&#x2F;dmk^1&#x2F;2^)进行拆分，实际上之前的softmax计算是以行为单位的：\n$$\\begin{equation}m(x):&#x3D;\\max _i x_i, \\quad f(x):&#x3D;\\left[\\begin{array}{lll}e^{x_1-m(x)} &amp; \\ldots &amp; e^{x_B-m(x)}\\end{array}\\right], \\quad \\ell(x):&#x3D;\\sum_i f(x)_i, \\quad \\operatorname{softmax}(x):&#x3D;\\frac{f(x)}{\\ell(x)}\\end{equation}$$\n在Flash_attention中，则对输入进行了分片：\n$$\\begin{equation}\\begin{aligned}&amp; m(x)&#x3D;m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)&#x3D;\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right), \\quad f(x)&#x3D;\\left[\\begin{array}{ll}e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) &amp; e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\end{array}\\right], \\&amp; \\ell(x)&#x3D;\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)&#x3D;e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right), \\quad \\operatorname{softmax}(x)&#x3D;\\frac{f(x)}{\\ell(x)} .\\end{aligned}\\end{equation}$$\n对Flash_Attention的前向和反向传播具体计算数学公式参见：https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh\nPagedAttention（这一部分主要参考链接https://readpaper.feishu.cn/docx/EcZxdsf4uozCoixdU3NcW03snwV： 感谢杨同学）\n当大模型进行推理时，将上一次生成的KV进行缓存产生KVcache能够加速大模型的生成速度\n\n但在现有的KV缓存技术中，缓存仍然存在占用较大，且大小取决于序列长度，不可预测的缺点。这样就造成了内存浪费十分严重（60%到80%）。因此，PagedAttention中将操作系统中的虚拟内存和分页思想引入至注意力算法中。PagedAttention主要有内存布局以固定大小的页为单位，可以进行内存共享的特点\n内存布局对于一次kv计算而言，由于引入了分页机制，其所用的kvcache不再需要在一段连续的内存中。因此内存的利用效率会大大提升（浪费率降低至4%左右），带来batchsize的大大提升，随之而来GPU的利用效率大大提升，显著提升吞吐量\n内存共享在并行采样中，从相同的提示生成多个输出序列。在这种情况下，可以在输出序列之间共享提示的计算和内存。通过其块表，PagedAttention能够自然地实现内存共享。类似于进程共享物理页，PagedAttention中的不同序列可以通过将它们的逻辑块映射到相同的物理块来共享块。为确保安全共享，PagedAttention跟踪物理块的引用计数并实现 Copy-on-Write 机制。\n通过PagedAttention的内存共享机制，极大地降低了复杂采样算法（如ParallelSampling和BeamSearch）的内存开销，使其内存使用量下降了高达55%。这项优化可以直接带来最多2.2倍的吞吐量提升，从而使得LLM服务中使用这些采样方法变得更加实用。\nAttention本身：手撕多头注意力机制：参见https://nn.labml.ai/transformers/mha.html\n这里做个备注：\n137        assert mask.shape[0] &#x3D;&#x3D; 1 or mask.shape[0] &#x3D;&#x3D; query_shape[0]\n138        assert mask.shape[1] &#x3D;&#x3D; key_shape[0]\n139        assert mask.shape[2] &#x3D;&#x3D; 1 or mask.shape[2] &#x3D;&#x3D; query_shape[1]\n这些断言检查传入的 mask 张量是否具有预期的形状：\n行 137: 确保掩码的第一个维度要么是1（表示同一个掩码应用于所有查询），要么与查询序列的长度相同。\n行 138: 确保掩码的第二个维度与键的序列长度相匹配。\n行 139: 确保掩码的第三个维度要么是1（表示同一个掩码应用于所有批次中的查询），要么与批次大小相同。\n大模型与训练显存量预估有两种公式（其中P代表模型参数，B代表batchsize,A代表优化器状态）\n（较为精准）\n总显存&#x3D;(P+B×A+P+2×P)×4字节\n（较为粗略）\n进一步简化一下只考虑模型的参数和批次大小，那么大约是4倍批次大小乘以参数的关系。\n大模型参数量分析N &#x3D; 2d_modeln_layers(2*d_attn+dff)\nPEFT方面QLORAQlora中只需要掌握三个部分\n\nNF4 数据类型\n\n计算公式：\n\nDouble Quant\n\n对原模型的参数QLoRA 将每 64 个参数为做一个 block，即 block_size &#x3D; 64，每个 block 计算一个 Scale。由于量化后的 Scale 通常以 FP32 存储，在 block 数众多的情况下，Scale 占用的显存也不可忽视。因此，QLoRA 对 Scale 进一步量化成 FP8，取 Double Quant 的 block size &#x3D; 256，因而进一步降低了显存消耗。\n\nPaged Optimizers\n\n允许Optimizers在显存不够用的情况下offload到内存中，之后再上来\nDoraDoRA（Weight-Decomposed Low-Rank Adaptation）的主要思想是将预训练权重分解为幅度（magnitude）和方向（direction），并利用LoRA来微调方向矩阵\n\nii. 为什么会出现 LLMs 复读机问题？出现LLMs复读机问题可能有以下几个原因：\n数据偏差：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。\n训练目标的限制：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现。\n缺乏多样性的训练数据：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性，导致复读机问题的出现。\n模型结构和参数设置：大型语言模型的结构和参数设置也可能对复读机问题产生影响。例如，模型的注意力机制和生成策略可能导致模型更倾向于复制输入的文本。为了解决复读机问题，可以采取以下策略：\n多样性训练数据：在训练阶段，尽量使用多样性的语料库来训练模型，避免数据偏差和重复文本的问题。\n引入噪声：在生成文本时，可以引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。\n温度参数调整：温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性，从而减少复读机问题的出现。后处理和过滤：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。\n需要注意的是，复读机问题是大型语言模型面临的一个挑战，解决这个问题是一个复杂的任务，需要综合考虑数据、训练目标、模型架构和生成策略等多个因素。目前，研究人员和工程师们正在不断努力改进和优化大型语言模型，以提高其生成文本的多样性和创造性。\n","slug":"LLM八股文（整理自用_不保证正确）","date":"2024-04-12T11:40:39.386Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"7be461416c2d0f0593235527ffa02f82","title":"leetcode刷题核心点记录","content":"leetcode 470 用 Rand7() 实现 Rand10()已知 rand_N() 可以等概率的生成[1, N]范围的随机数那么：(rand_X() - 1) × Y + rand_Y() &#x3D;&#x3D;&gt; 可以等概率的生成[1, X * Y]范围的随机数即实现了 rand_XY()\n拒绝采样：如果某个采样结果不在要求的范围内，则丢弃它\n优化：利用不在采样范围内的数据生成采样范围内的数据（目标的倍数）将rand_y()进行灵活转化。\n","slug":"leetcode_核心点记录","date":"2024-03-30T11:38:46.466Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"80a7e21722935a33527e2b073eb00486","title":"2024_3_29记录(秒凉)","content":"第一题 c++,java 基础题脑抽答错了，记得多nk的计算机基础\n第二题 编译型和非编译型SAST的区别答的不是很好，以下是glm4给出的答案\n编译型静态应用程序安全测试（SAST）和非编译型静态应用程序安全测试（SAST）是两种不同的安全测试方法，它们在应用程序的安全评估过程中有不同的应用场景和特点。\n\n编译型SAST：\n编译型SAST工具在编译阶段对源代码进行分析，它需要访问应用程序的源代码。\n这些工具通常可以识别代码中的安全漏洞，如缓冲区溢出、SQL注入等。\n编译型SAST工具可以提供更精确和深入的安全分析，因为它们可以理解代码的结构和逻辑。\n编译型SAST工具通常需要更多的配置和设置，以便正确地集成到开发过程中。\n\n\n非编译型SAST：\n非编译型SAST工具在源代码或字节码上进行操作，不需要编译过程。\n这些工具通常可以检测到源代码中的安全漏洞，如跨站脚本攻击（XSS）、跨站请求伪造（CSRF）等。\n非编译型SAST工具通常更容易集成到开发过程中，因为它们不需要编译步骤。\n非编译型SAST工具可能不如编译型工具那样精确，因为它们不能完全理解代码的结构和逻辑。总的来说，编译型和非编译型SAST工具都有其优缺点，选择哪种工具取决于具体的安全需求和开发环境。在实际应用中，可以将这两种工具结合使用，以获得更全面的安全评估。\n\n\n\n拓展：SAST，DAST和IAST到底是什么？他们之间的优劣势如何？（转载自https://zhuanlan.zhihu.com/p/98007493；作者：DevOps安全社）一、SAST\nSAST（Static Application Security Testing，静态应用程序安全测试）对应用程序源代码执行直接的白盒分析。分析是在代码的静态视图上运行的，这意味着代码在审查时没有运行。如今，SAST已经完全成为主流，并且在整个软件行业中被广泛采用。\nSAST的优点：\n广泛的编程语言支持；\n检出率较高；\n可以定位到代码行。\nSAST的缺点：\n准确性差：优秀SAST产品的误报率也在53%以上*；\n无法看到执行流；\n通常需要一些定制或调参；\n不适用于生产阶段的系统；\n通常运行很慢。\n二、DAST\n与SAST相反，DAST（Dynamic Application Security Testing，动态应用程序安全测试）对应用程序进行黑盒分析，这意味着它们不能访问代码或实现细节。DAST只检查系统对潜在漏洞测试的请求和响应。换言之，DAST是外部的漏洞扫描程序。\nDAST的优点：\n独立于应用程序的技术和平台，无需代码细节；\n执行相对较快；\n误报率较低。\nDAST的缺点：\n检出率低：优秀的DAST产品检出率也只有18%*；\n无法定位到代码行；\n使用门槛高，报告通常需要安全专家解读。\n三、IAST\nIAST（Interactive Application Security Testing，交互式应用程序安全测试）结合了SAST和DAST的优点。IAST可以像SAST一样看到源代码，也可以像DAST一样看到应用程序运行时的执行流。\nIAST的优点：\n检出率较高；\n误报率较低；\n可以在研发测试和生产环境中使用；\n实时产生结果；\n可以持续检测，对DevOps支持度更高；\n即插即用，无需配置或调参；\n可以与CI平台集成，创建相互连接的工作流。\nIAST的缺点：\n需要特定的语言支持\n\n第三题 IDE为何可以在用户编写代码而未运行时报出错误提示；基本上答出来，但太拘泥于细节。\n规则和模式匹配：静态分析工具通常基于一系列预定义的规则和模式来检查代码。这些规则可以是关于代码风格、编程语言的语法、可能的错误或安全漏洞的。工具会扫描代码，寻找与这些规则匹配的模式，并标记出潜在的问题。代码质量检查：静态分析工具可以检测代码中的质量问题，如重复的代码、过于复杂的函数、过长的文件、循环依赖等。这些质量问题可能会影响代码的可维护性和可读性。错误检测：工具可以识别可能导致运行时错误的代码，如空指针引用、数组越界访问、除以零等。通过在开发早期发现这些错误，可以减少调试和修复错误的时间。安全漏洞扫描：静态分析工具可以检查代码中可能的安全漏洞，如SQL注入、跨站脚本（XSS）、缓冲区溢出等。这些漏洞可能会被攻击者利用，导致安全风险。\n第四题 codeql是否可以被修改未无需编译的工具回答组织错乱，以下是我重新组织的回答（不保证正确）\n如果是从原理上来说，codeql本身对特定语言（Python,js等）无需编译，其仅仅倾向于使用编译的方法来获取更多信息，因此codeql可以被修改为无需编译的工具，以JAVA为例，codeql在提取其AST时将其分为stmt.表达式，Map类型等，这些信息都可以使用非编译的工具来提取。但不用编译的方法来提取这些信息，相较于编译方法而言准确度会降低。这就是为什么codeql尽量使用编译的原因。\n第五题 了解哪些类型的漏洞建议参照OWASP Top10来回答，2021版本为：\nA01:2021-权限控制失效 从第五名移上來; 94% 被测试的应用程式都有验证到某种类别权限控制失效的问题。在权限控制失效这个类别中被对应到的 34 个 CWEs 在验测资料中出现的次数都高于其他的弱点类别。\nA02:2021-加密机制失效 提升一名到第二名，在之前为 敏感资料外曝，在此定义下比较类似于一个广泛的问题而非根本原因。在此重新定义并将问题核心定义在加密机制的失败，并因此造成敏感性资料外泄或是系統被破坏。\nA03:2021-注入式攻击 下滑到第三名。94% 被测试的应用程式都有验测到某种类別注入式攻击的问题。在注入式攻击这个类別中被对应到的 33 个 CWEs 在验测资料中出现的次数为弱点问题的第二高。跨站脚本攻击现在在新版本属于这个类別。\nA04:2021-不安全设计 这是 2021 年版本的新类別，并特別聚焦在设计相关的缺陷。如果我们真的希望让整个产业”向左移动”＊注一＊，那我们必须进一步的往威胁建模，安全设计模块的观念，和安全參考架构前进。\n＊注一: Move Left 于英文原文中代表在软件开发及交付过程中，在早期找出及处理相关问题，同 Shift Left Testing。＊\nA05:2021-安全设定缺陷 从上一版本的第六名移动上來。90% 被测试的应用程式都有验测到某种类別的安全设定缺陷。在更多的软件往更高度和有弹性的设定移动，我们并不意外这个类別的问题往上移动。在前版本中的 XML 外部实体注入攻击 （XML External Entities）现在属于这个类別。\nA06:2021-危险或过旧的组件 在之前标题为 使用有已知弱点的组件。在本次版本中于业界问卷中排名第二，但也有足够的统计资料让它可以进入 Top 10。这个类別从 2017 版本的第九名爬升到第六，也是我们持续挣扎做测试和评估风险的类別。这也是唯一一个沒有任何 CVE 能被对应到 CWE 內的类別，所以预设的威胁及影响权重在这类別的分数上被预设为 5.0。\nA07:2021-认证及验证机制失效 在之前标题为 错误的认证机制。在本次版本中由第二名下滑至此，并同时包含了将认证相关缺失的 CWE 包含在內。这个类別仍是 Top 10 不可缺少的一环，但同时也有发现现在标准化的架构有协助降低次风险发生机率。\nA08:2021-软件及资料完整性失效 这是 2021 年版本全新的类別，并在软件更新，敏感及重要资料，和 CI&#x2F;CD 管道中并沒有做完整性的确认为前提做假设并进行评估。在评估中影响权重最高分的 CVE&#x2F;CVSS 资料都与这类別中的 10 个 CWE 对应到。2017 年版本中不安全的反序列化现在被合并至此类別。\nA09:2021-安全记录及监控失效 在之前为不完整的记录及监控并纳入在业界问卷中在本次列名为第三名并从之前的第十名上移。这个类別将扩充去纳入更多相关的缺失，但这也是相当难去验证，并沒有相当多的 CVE&#x2F;CVSS 资料可以佐证。但是在这个类別中的缺失会直接影响到整体安全的可视性，事件告警及取证。\nA10:2021-服务器端请求伪造 这个类別是在业界问卷排名第一名，并在此版本內纳入。由资料显示此问题有较低被验测次数和范围，但有高于平均的威胁及影响权重比率。这个类別的出现也是因为业界专家重复申明这类別的问题相当重要，即使在本次资料中并沒有足够的资料去显示这个问题。\n第六题 污点分析：\n\n\n2024年3月30日更新：\n发现问过的问题很多来源于https://security.tencent.com/index.php/blog/msg/191只能说之后最好多看相关博客，哭笑不得了属于是\n","slug":"2024_3_29记录(秒挂)","date":"2024-03-30T08:46:44.216Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"2c99110fb374d2251c5f6902cacac226","title":".vscode出现fail_to_download_vs_code_server解决办法（转载）","content":"查看失败日志，锁定commit id到外网环境下载：curl -sSL “https://update.code.visualstudio.com/commit:${commit_id}/server-linux-x64/stable” -o vscode-server-linux-x64.tar.gzmkdir -p ~&#x2F;.vscode-server&#x2F;bin&#x2F;${commit_id}tar zxvf &#x2F;tmp&#x2F;vscode-server-linux-x64.tar.gz -C ~&#x2F;.vscode-server&#x2F;bin&#x2F;${commit_id} –strip 1touch ~&#x2F;.vscode-server&#x2F;bin&#x2F;${commit_id}&#x2F;0\n","slug":"vscode出现fail_to_download_vs_code_server解决办法","date":"2024-03-25T13:27:56.055Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"e60c2d9ec695dd65b5dadea96fa55318","title":"如何kill 掉pid一直变化的进程","content":"kill $(ps aux | grep java | tr -s &#39; &#39;| cut -d &#39; &#39; -f 2)其中，java可以替换为其他词\n命令解析：\n\n$(ps aux | grep java)：使用ps命令列出所有正在运行的进程，并使用grep命令筛选出包含”java”字符串的行。\ntr -s &#39; &#39;：将筛选出的行中的空格替换为空字符，以便后续的命令能够正确处理。\ncut -d &#39; &#39; -f 2：使用cut命令从每行中提取第二个字段，即进程ID（PID）\n\ntr命令tr命令是Unix&#x2F;Linux系统中的一个字符替换工具，用于在文本中替换指定的字符或字符集。它的基本语法如下：\n1tr [OPTIONS] SET1 SET2\n\n其中，SET1和SET2是两个字符集，tr命令会将SET1中的每个字符替换为SET2中对应的字符。如果省略SET2，则默认使用空字符替换SET1中的每个字符。\ntr命令的选项包括：\n\n-c：指定替换次数，即最多替换多少个字符。\n-d：指定删除字符集，即删除SET1中的字符。\n-s：指定替换字符集，即SET1中的字符替换为SET2中的字符。\n-t：指定替换字符集，即SET1中的字符替换为SET2中的字符，但只替换第一个匹配项。\n-u：指定替换字符集，即SET1中的字符替换为SET2中的字符，但只替换第一个匹配项，并且删除SET1中的字符。\n\ntr命令常用于文本处理、数据转换等场景，例如将文件中的换行符替换为制表符、将大小写字母互换等。\ncut命令cut命令是Unix&#x2F;Linux系统中的一个文本处理工具，用于从文本中提取指定位置的字段。它的基本语法如下：\n1cut [OPTIONS] [FILE] [START END]\n\n其中，OPTIONS是可选参数，用于指定输出格式、分隔符等；FILE是可选参数，用于指定要处理的文本文件；START和END是可选参数，用于指定要提取的字段范围。\ncut命令的选项包括：\n\n-b：指定分隔符，默认为制表符。\n-c：指定要提取的字段数，例如-c 2表示提取第2个字段。\n-d：指定分隔符，默认为制表符。\n-f：指定输出格式，例如-f 1,3表示输出第1个和第3个字段。\n-s：指定分隔符，默认为制表符。\n-t：指定分隔符，默认为制表符。\n\ncut命令常用于文本处理、数据提取等场景，例如从日志文件中提取IP地址、从CSV文件中提取特定列等。\nKill命令kill $()是一个特殊的命令，用于杀死当前shell进程。具体来说，$()表示当前shell进程的进程ID，因此kill $()会杀死当前shell进程。\n需要注意的是，kill $()命令只能杀死当前shell进程，而不能杀死其他进程。如果需要杀死其他进程，可以使用kill命令，例如kill 1234表示杀死进程ID为1234的进程。\n另外，需要注意的是，kill命令是一种非常危险的操作，如果不小心杀死了一个重要的进程，可能会导致系统崩溃或数据丢失。因此，在使用kill命令时，一定要谨慎操作，确保不会误杀重要进程。\n","slug":"如何kill 掉pid一直变化的进程","date":"2024-02-28T15:16:11.000Z","categories_index":"Daily record","tags_index":"Daily record","author_index":"Shuo Hong"},{"id":"247ac1e7bd99591b6c3fd2ab7eabb900","title":"Hex简易命令（转载）","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post1$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server1$ hexo server\n\nMore info: Server\nGenerate static files1$ hexo generate\n\nMore info: Generating\nDeploy to remote sites1$ hexo deploy\n\nMore info: Deployment\n","slug":"Hexo建议命令","date":"2024-02-27T16:17:50.465Z","categories_index":"","tags_index":"","author_index":"Shuo Hong"},{"id":"801591854ef42c23b79225abd4e53d59","title":"LLM简单评测_2024_2_26","content":"#traing_of_fintuning使用freeze进行微调，目前微调结果如下：微调基于&#x2F;llm&#x2F;LLaMA-Factory&#x2F;zephyr-7b-lora-export，各个情况下的评估代码以及评估结果如下所示：\nzephyr-7b-alpha评估代码：CUDA_VISIBLE_DEVICES&#x3D;0 python src&#x2F;evaluate.py     –model_name_or_path HuggingFaceH4&#x2F;zephyr-7b-alpha  –template vanilla     –quantization_bit 4     –task ceval     –split validation     –lang zh     –n_shot 5     –batch_size 4\n评估结果：Average: 43.24STEM: 44.65Social Sciences: 46.18Humanities: 42.02Other: 40.36\n&#x2F;llm&#x2F;LLaMA-Factory&#x2F;zephyr-7b-lora-export，评估代码：CUDA_VISIBLE_DEVICES&#x3D;0 python src&#x2F;evaluate.py     –model_name_or_path &#x2F;llm&#x2F;LLaMA-Factory&#x2F;zephyr-7b-lora-export  –template vanilla     –quantization_bit 4     –task ceval     –split validation     –lang zh     –n_shot 5     –batch_size 4 \n评估结果：Average: 41.53                                                                                                                  STEM: 40.47Social Sciences: 46.18Humanities: 42.80Other: 38.54\n&#x2F;llm&#x2F;LLaMA-Factory&#x2F;saves&#x2F;Zephyr-7B-Alpha-Chat&#x2F;freeze&#x2F;train_2024-01-01-23-13 （本次）评估代码：CUDA_VISIBLE_DEVICES&#x3D;0,1 accelerate launch  src&#x2F;evaluate.py     –model_name_or_path &#x2F;llm&#x2F;LLaMA-Factory&#x2F;saves&#x2F;Zephyr-7B-Alpha-Chat&#x2F;freeze&#x2F;train_2024-01-01-23-13 –template vanilla    –quantization_bit 4     –task ceval     –split validation     –lang zh     –n_shot 5     –batch_size 4评估结果：Average: 43.16                                                                                                                  STEM: 42.56Social Sciences: 48.00 Humanities: 44.36  Other: 39.58\n下一步：freeze微调相应代码","slug":"LLM简单评测","date":"2024-02-27T15:25:11.000Z","categories_index":"Daily record","tags_index":"Daily record","author_index":"Shuo Hong"},{"id":"aadb4a19073e41a58c91edd26f1f5046","title":"如何使用deepspeed进行多机多卡训练（简易版本）_2024_2_27","content":"deepspeed多机多卡搭建记录（简易）\n0 docker网络搭建https://www.jianshu.com/p/2792ebf9af71\n1 配置免密登录https://blog.csdn.net/qq_40732354/article/details/95061228\n2 deepspeed配置https://www.jianshu.com/p/2792ebf9af71注：hostfile为 ip slot=[gpu数量]即可\nbug 解决方案deepspeed 报错 up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store 解决https://blog.csdn.net/qq874455953/article/details/134408257\n","slug":"how-to-use-deepspeed-on-many-servers-record-2024-2-27 copy","date":"2024-02-27T15:25:11.000Z","categories_index":"Daily record","tags_index":"Daily record","author_index":"Shuo Hong"}]