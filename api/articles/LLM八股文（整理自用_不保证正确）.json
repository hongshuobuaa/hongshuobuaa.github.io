{"title":"LLM八股文（整理自用_不保证正确）（持续更新）","uid":"542f169762b7aca64f020ec0392added","slug":"LLM八股文（整理自用_不保证正确）","date":"2024-04-12T11:40:39.386Z","updated":"2024-04-15T00:34:47.121Z","comments":true,"path":"api/articles/LLM八股文（整理自用_不保证正确）.json","keywords":null,"cover":"https://raw.githubusercontent.com/hexojs/logo/master/hexo-logo-avatar.png","content":"<h1 id=\"Attention改进方面\"><a href=\"#Attention改进方面\" class=\"headerlink\" title=\"Attention改进方面\"></a>Attention改进方面</h1><h2 id=\"Flash-Attention\"><a href=\"#Flash-Attention\" class=\"headerlink\" title=\"Flash_Attention\"></a>Flash_Attention</h2><p><strong>（这一部分主要参考链接：<a href=\"https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh\">https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh</a> 感谢杨同学）</strong></p>\n<p>在传统的Attention中，softmax(QK^T^&#x2F;dm<del>k</del>^1&#x2F;2^)中的矩阵运算很大，需要在HBM中实例化存储，因此会带来很多的对显存的<br>访存操作，导致耗时较长。</p>\n<p>FlashAttention则是将对QK的softmax计算进行了分片，每次进行部分计算时。计算都能在SRAM中进行交互，从而减少对显存的依赖。</p>\n<p>让我们将softmax(QK^T^&#x2F;dm<del>k</del>^1&#x2F;2^)进行拆分，实际上之前的softmax计算是以行为单位的：</p>\n<p>$$\\begin{equation}<br>m(x):&#x3D;\\max _i x_i, \\quad f(x):&#x3D;\\left[\\begin{array}{lll}<br>e^{x_1-m(x)} &amp; \\ldots &amp; e^{x_B-m(x)}<br>\\end{array}\\right], \\quad \\ell(x):&#x3D;\\sum_i f(x)_i, \\quad \\operatorname{softmax}(x):&#x3D;\\frac{f(x)}{\\ell(x)}<br>\\end{equation}$$</p>\n<p>在Flash_attention中，则对输入进行了分片：</p>\n<p>$$\\begin{equation}<br>\\begin{aligned}<br>&amp; m(x)&#x3D;m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)&#x3D;\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right), \\quad f(x)&#x3D;\\left[\\begin{array}{ll}<br>e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) &amp; e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)<br>\\end{array}\\right], \\<br>&amp; \\ell(x)&#x3D;\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)&#x3D;e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right), \\quad \\operatorname{softmax}(x)&#x3D;\\frac{f(x)}{\\ell(x)} .<br>\\end{aligned}<br>\\end{equation}<br>$$</p>\n<p>对Flash_Attention的前向和反向传播具体计算数学公式参见：<a href=\"https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh\">https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh</a></p>\n<h2 id=\"PagedAttention\"><a href=\"#PagedAttention\" class=\"headerlink\" title=\"PagedAttention\"></a>PagedAttention</h2><p><strong>（这一部分主要参考链接<a href=\"https://readpaper.feishu.cn/docx/EcZxdsf4uozCoixdU3NcW03snwV%EF%BC%9A\">https://readpaper.feishu.cn/docx/EcZxdsf4uozCoixdU3NcW03snwV：</a> 感谢杨同学）</strong></p>\n<p>当大模型进行推理时，将上一次生成的KV进行缓存产生KVcache能够加速大模型的生成速度</p>\n<p><img src=\"/./image-2.png\" alt=\"alt text\"></p>\n<p>但在现有的KV缓存技术中，缓存仍然存在占用较大，且大小取决于序列长度，不可预测的缺点。这样就造成了内存浪费十分严重（60%到80%）。因此，PagedAttention中将操作系统中的虚拟内存和分页思想引入至注意力算法中。PagedAttention主要有内存布局以固定大小的页为单位，可以进行内存共享的特点</p>\n<h3 id=\"内存布局\"><a href=\"#内存布局\" class=\"headerlink\" title=\"内存布局\"></a>内存布局</h3><p>对于一次kv计算而言，由于引入了分页机制，其所用的kvcache不再需要在一段连续的内存中。因此内存的利用效率会大大提升（浪费率降低至4%左右），带来batchsize的大大提升，随之而来GPU的利用效率大大提升，显著提升吞吐量</p>\n<h3 id=\"内存共享\"><a href=\"#内存共享\" class=\"headerlink\" title=\"内存共享\"></a>内存共享</h3><p>在并行采样中，从相同的提示生成多个输出序列。在这种情况下，可以在输出序列之间共享提示的计算和内存。通过其块表，PagedAttention能够自然地实现内存共享。类似于进程共享物理页，PagedAttention中的不同序列可以通过将它们的逻辑块映射到相同的物理块来共享块。为确保安全共享，PagedAttention跟踪物理块的引用计数并实现 Copy-on-Write 机制。</p>\n<p>通过PagedAttention的内存共享机制，极大地降低了复杂采样算法（如ParallelSampling和BeamSearch）的内存开销，使其内存使用量下降了高达55%。这项优化可以直接带来最多2.2倍的吞吐量提升，从而使得LLM服务中使用这些采样方法变得更加实用。</p>\n<h2 id=\"Attention本身：\"><a href=\"#Attention本身：\" class=\"headerlink\" title=\"Attention本身：\"></a>Attention本身：</h2><h2 id=\"手撕多头注意力机制：\"><a href=\"#手撕多头注意力机制：\" class=\"headerlink\" title=\"手撕多头注意力机制：\"></a>手撕多头注意力机制：</h2><p>参见<a href=\"https://nn.labml.ai/transformers/mha.html\">https://nn.labml.ai/transformers/mha.html</a></p>\n<p>这里做个备注：</p>\n<p>137        assert mask.shape[0] &#x3D;&#x3D; 1 or mask.shape[0] &#x3D;&#x3D; query_shape[0]</p>\n<p>138        assert mask.shape[1] &#x3D;&#x3D; key_shape[0]</p>\n<p>139        assert mask.shape[2] &#x3D;&#x3D; 1 or mask.shape[2] &#x3D;&#x3D; query_shape[1]</p>\n<p>这些断言检查传入的 mask 张量是否具有预期的形状：</p>\n<p>行 137: 确保掩码的第一个维度要么是1（表示同一个掩码应用于所有查询），要么与查询序列的长度相同。</p>\n<p>行 138: 确保掩码的第二个维度与键的序列长度相匹配。</p>\n<p>行 139: 确保掩码的第三个维度要么是1（表示同一个掩码应用于所有批次中的查询），要么与批次大小相同。</p>\n<h1 id=\"大模型与训练显存量预估\"><a href=\"#大模型与训练显存量预估\" class=\"headerlink\" title=\"大模型与训练显存量预估\"></a>大模型与训练显存量预估</h1><p>有两种公式（其中P代表模型参数，B代表batchsize,A代表优化器状态）</p>\n<p>（较为精准）</p>\n<p>总显存&#x3D;(P+B×A+P+2×P)×4字节</p>\n<p>（较为粗略）</p>\n<p>进一步简化一下只考虑模型的参数和批次大小，那么大约是4倍批次大小乘以参数的关系。</p>\n<h2 id=\"大模型参数量分析\"><a href=\"#大模型参数量分析\" class=\"headerlink\" title=\"大模型参数量分析\"></a>大模型参数量分析</h2><p>N &#x3D; 2<em>d_model</em>n_layers(2*d_attn+dff)</p>\n<h1 id=\"PEFT方面\"><a href=\"#PEFT方面\" class=\"headerlink\" title=\"PEFT方面\"></a>PEFT方面</h1><h2 id=\"QLORA\"><a href=\"#QLORA\" class=\"headerlink\" title=\"QLORA\"></a>QLORA</h2><p>Qlora中只需要掌握三个部分</p>\n<ul>\n<li>NF4 数据类型</li>\n</ul>\n<p>计算公式：<br><img src=\"/image-3.png\" alt=\"alt text\"></p>\n<ul>\n<li>Double Quant</li>\n</ul>\n<p>对原模型的参数<br>QLoRA 将每 64 个参数为做一个 block，即 block_size &#x3D; 64，每个 block 计算一个 Scale。由于量化后的 Scale 通常以 FP32 存储，在 block 数众多的情况下，Scale 占用的显存也不可忽视。因此，QLoRA 对 Scale 进一步量化成 FP8，取 Double Quant 的 block size &#x3D; 256，因而进一步降低了显存消耗。</p>\n<ul>\n<li>Paged Optimizers</li>\n</ul>\n<p>允许Optimizers在显存不够用的情况下offload到内存中，之后再上来</p>\n<h2 id=\"Dora\"><a href=\"#Dora\" class=\"headerlink\" title=\"Dora\"></a>Dora</h2><p>DoRA（Weight-Decomposed Low-Rank Adaptation）的主要思想是将预训练权重分解为幅度（magnitude）和方向（direction），并利用LoRA来微调方向矩阵</p>\n<p><img src=\"/image-4.png\" alt=\"alt text\"></p>\n<h2 id=\"ii-为什么会出现-LLMs-复读机问题？\"><a href=\"#ii-为什么会出现-LLMs-复读机问题？\" class=\"headerlink\" title=\"ii. 为什么会出现 LLMs 复读机问题？\"></a>ii. 为什么会出现 LLMs 复读机问题？</h2><p>出现LLMs复读机问题可能有以下几个原因：</p>\n<p>数据偏差：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。</p>\n<p>训练目标的限制：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现。</p>\n<p>缺乏多样性的训练数据：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性，导致复读机问题的出现。</p>\n<p>模型结构和参数设置：大型语言模型的结构和参数设置也可能对复读机问题产生影响。例如，模型的注意力机制和生成策略可能导致模型更倾向于复制输入的文本。<br>为了解决复读机问题，可以采取以下策略：</p>\n<p>多样性训练数据：在训练阶段，尽量使用多样性的语料库来训练模型，避免数据偏差和重复文本的问题。</p>\n<p>引入噪声：在生成文本时，可以引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。</p>\n<p>温度参数调整：温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性，从而减少复读机问题的出现。<br>后处理和过滤：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。</p>\n<p>需要注意的是，复读机问题是大型语言模型面临的一个挑战，解决这个问题是一个复杂的任务，需要综合考虑数据、训练目标、模型架构和生成策略等多个因素。目前，研究人员和工程师们正在不断努力改进和优化大型语言模型，以提高其生成文本的多样性和创造性。</p>\n","feature":false,"text":"Attention改进方面Flash_Attention（这一部分主要参考链接：https://readpaper.feishu.cn/docx/AC7JdtL...","permalink":"/post/LLM八股文（整理自用_不保证正确）","photos":[],"count_time":{"symbolsCount":"3.9k","symbolsTime":"4 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Attention%E6%94%B9%E8%BF%9B%E6%96%B9%E9%9D%A2\"><span class=\"toc-text\">Attention改进方面</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Flash-Attention\"><span class=\"toc-text\">Flash_Attention</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#PagedAttention\"><span class=\"toc-text\">PagedAttention</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80\"><span class=\"toc-text\">内存布局</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%85%E5%AD%98%E5%85%B1%E4%BA%AB\"><span class=\"toc-text\">内存共享</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Attention%E6%9C%AC%E8%BA%AB%EF%BC%9A\"><span class=\"toc-text\">Attention本身：</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%89%8B%E6%92%95%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%9A\"><span class=\"toc-text\">手撕多头注意力机制：</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E9%87%8F%E9%A2%84%E4%BC%B0\"><span class=\"toc-text\">大模型与训练显存量预估</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90\"><span class=\"toc-text\">大模型参数量分析</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#PEFT%E6%96%B9%E9%9D%A2\"><span class=\"toc-text\">PEFT方面</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#QLORA\"><span class=\"toc-text\">QLORA</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Dora\"><span class=\"toc-text\">Dora</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#ii-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F\"><span class=\"toc-text\">ii. 为什么会出现 LLMs 复读机问题？</span></a></li></ol></li></ol>","author":{"name":"Shuo Hong","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/65355046?v=4","link":"/","description":"你好，我是来自北京航空航天大学的洪硕","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"LLM_LangChain八股文（整理自用_不保证正确）（持续更新）","uid":"33d3a0a2cbefdcf717ea2b53096214d6","slug":"LLM_langchain八股文","date":"2024-04-14T14:21:58.196Z","updated":"2024-04-15T01:52:41.669Z","comments":true,"path":"api/articles/LLM_langchain八股文.json","keywords":null,"cover":"https://raw.githubusercontent.com/hexojs/logo/master/hexo-logo-avatar.png","text":"LangChain本身什么是 LangChain?LangChain 是一个开发由语言模型驱动的应用程序的框架。我们相信最强大和不同的应用程序不仅会通过 API...","permalink":"/post/LLM_langchain八股文","photos":[],"count_time":{"symbolsCount":"4.7k","symbolsTime":"4 mins."},"categories":[],"tags":[],"author":{"name":"Shuo Hong","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/65355046?v=4","link":"/","description":"你好，我是来自北京航空航天大学的洪硕","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":false},"next_post":{"title":"leetcode刷题核心点记录","uid":"7be461416c2d0f0593235527ffa02f82","slug":"leetcode_核心点记录","date":"2024-03-30T11:38:46.466Z","updated":"2024-03-30T11:56:43.460Z","comments":true,"path":"api/articles/leetcode_核心点记录.json","keywords":null,"cover":null,"text":"leetcode 470 用 Rand7() 实现 Rand10()已知 rand_N() 可以等概率的生成[1, N]范围的随机数那么：(rand_X() -...","permalink":"/post/leetcode_核心点记录","photos":[],"count_time":{"symbolsCount":231,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Shuo Hong","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/65355046?v=4","link":"/","description":"你好，我是来自北京航空航天大学的洪硕","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":false}}