{"title":"AIGC信息随笔_2024_4_28","uid":"80b3295c405f37b732a74987a11f156e","slug":"AIGC信息随笔_2024_4_28","date":"2024-04-28T07:16:43.950Z","updated":"2024-04-28T07:25:29.045Z","comments":true,"path":"api/articles/AIGC信息随笔_2024_4_28.json","keywords":null,"cover":null,"content":"<h1 id=\"Huggingface-发布-fineweb-预训练数据集\"><a href=\"#Huggingface-发布-fineweb-预训练数据集\" class=\"headerlink\" title=\"Huggingface 发布 fineweb 预训练数据集\"></a>Huggingface 发布 fineweb 预训练数据集</h1><p>FineWeb是一个由Hugging Face提供的大规模英语网页数据集，包含超过15万亿个经过清洗和去重的token，源自CommonCrawl。该数据集旨在为大型语言模型（LLM）的训练提供优化的数据处理流程，并使用datatrove库进行处理。FineWeb的性能已超越了RefinedWeb等其他高质量网络数据集。</p>\n<p>整个数据集有45TB大小，但可以自行选在从2013至2024年的数据进行训练</p>\n<p>看数据应该是一个很好的数据集，但实在太大了。或许可以取用其中的一部分进行训练？或者像llama3一样，选一个好一些的模型清洗一遍再预训练或微调？（正好用qwen1.5-110B清洗（笑））</p>\n<h1 id=\"Qwen1-5-110B发布\"><a href=\"#Qwen1-5-110B发布\" class=\"headerlink\" title=\"Qwen1.5 110B发布\"></a>Qwen1.5 110B发布</h1><p>此模型为qwen发布以来尺寸最大的开源模型，支持32K上上下文。性能指标方面和llama3 72B相近，但中文能力更强。实测体验良好，GPTQ-INT4版本A100(80GB版本)单卡可以部署。未量化版本显存占用待测</p>\n","feature":true,"text":"Huggingface 发布 fineweb 预训练数据集FineWeb是一个由Hugging Face提供的大规模英语网页数据集，包含超过15万亿个经过清洗和...","permalink":"/post/AIGC信息随笔_2024_4_28","photos":[],"count_time":{"symbolsCount":448,"symbolsTime":"1 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Huggingface-%E5%8F%91%E5%B8%83-fineweb-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86\"><span class=\"toc-text\">Huggingface 发布 fineweb 预训练数据集</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Qwen1-5-110B%E5%8F%91%E5%B8%83\"><span class=\"toc-text\">Qwen1.5 110B发布</span></a></li></ol>","author":{"name":"Shuo Hong","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/65355046?v=4","link":"/","description":"你好，我是来自北京航空航天大学的洪硕","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"python和协程进行IO多路复用的例子","uid":"255c3f38c555c61c61771065e0b22e41","slug":"python_io多路复用样例","date":"2024-04-27T14:05:40.900Z","updated":"2024-05-06T08:29:23.296Z","comments":true,"path":"api/articles/python_io多路复用样例.json","keywords":null,"cover":null,"text":"使用Python进行IO多路复用的一个常见方法是使用asyncio库，这是Python中用于编写单线程并发代码的库，底层使用selectors模块实现IO多路复...","permalink":"/post/python_io多路复用样例","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Shuo Hong","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/65355046?v=4","link":"/","description":"你好，我是来自北京航空航天大学的洪硕","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}